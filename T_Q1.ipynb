{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### This cell deal with loading of data set and taking a peek into the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "def load_data_set() : \n",
    "    return pd.read_csv(\"/Users/pmanthan/Desktop/ML Practice /train.csv\")\n",
    "    \n",
    "data_set = load_data_set()\n",
    "data_set_valuecount = data_set['Category'].value_counts()\n",
    "data_set_valuecount.head()\n",
    "\n",
    "data_set.head()\n",
    "data_set[\"Category\"].value_counts()/len(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### This snippet divides the data into training and test data based on stratifies splitting so that the composure of the original data is maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for train_idx,test_idx in split.split(data_set,data_set[\"Category\"]) : \n",
    "    strat_train_data = data_set.loc[train_idx]\n",
    "    strat_test_data = data_set.loc[test_idx]\n",
    "    \n",
    "strat_train_data[\"Category\"].value_counts() / len(data_set)\n",
    "strat_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separates the target variables from the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_category = strat_train_data[\"Category\"].copy()\n",
    "strat_train_text = strat_train_data.drop([\"Category\"],axis=1)\n",
    "\n",
    "strat_train_text[\"Text\"] = strat_train_text[\"Text\"].str.lower()\n",
    "strat_train_text[\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this part of code i used regex for the text cleaning and spacy for the tokenization and stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "punctuation_pattern = r'[^\\w\\s$]'\n",
    "strat_train_text[\"Text\"] = strat_train_text[\"Text\"].str.replace(punctuation_pattern,'',regex=True)\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_data(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "strat_train_text[\"Tokens\"] = strat_train_text[\"Text\"].apply(tokenize_data)\n",
    "\n",
    "strat_train_text.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatixation of data into its base form using the spacy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_data (text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc ]\n",
    "\n",
    "strat_train_text[\"Tokens\"] = strat_train_text[\"Text\"].apply(lemmatize_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion of the text into word embeddings using the bert model and also splitting the data into training and validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text  \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "bert_preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "    name=\"bert_preprocessor\")\n",
    "bert_model = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
    "    name=\"bert_encoder\"\n",
    ")\n",
    "\n",
    "def bert_vectorization_batch(texts):\n",
    "    texts_tensor = tf.convert_to_tensor(texts, dtype=tf.string)\n",
    "\n",
    "    preprocessed = bert_preprocessor(texts_tensor)\n",
    "    outputs = bert_model(preprocessed)\n",
    "\n",
    "    return outputs[\"pooled_output\"]\n",
    "\n",
    "texts = strat_train_text[\"Tokens\"].astype(str).tolist()\n",
    "\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "    try:\n",
    "        batch_embeddings = bert_vectorization_batch(batch_texts)\n",
    "        all_embeddings.append(batch_embeddings.numpy())\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in batch {i}-{i+batch_size}: {e}\")\n",
    "all_embeddings_np = np.vstack(all_embeddings)\n",
    "\n",
    "strat_train_text[\"BERT_Embedding\"] = list(all_embeddings_np)\n",
    "\n",
    "strat_train_text_val = strat_train_text[10000:10712]\n",
    "strat_train_category_val = strat_train_category[10000:10712]\n",
    "strat_train_text_,strat_train_category_ = strat_train_text[:10000],strat_train_category[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This code deals with imbalance nature of the data set i found out the class weights so that the model than give more weight for the class with less number \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight \n",
    "\n",
    "class_weight = compute_class_weight(class_weight='balanced',classes = np.unique(data_set[\"Category\"]),y=data_set[\"Category\"])\n",
    "class_weight_dict = dict(enumerate(class_weight))\n",
    "print(\"Class weights\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data type checking due to some dtype error showed by ig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(strat_train_category_, pd.DataFrame)\n",
    "strat_train_category_.head()\n",
    "strat_train_category_val.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing the labels for the training of model encoding them into vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_int = label_encoder.fit_transform(strat_train_category_)\n",
    "y_val_int = label_encoder.transform(strat_train_category_val)\n",
    "\n",
    "y_train_cat = to_categorical(y_train_int)\n",
    "y_val_cat = to_categorical(y_val_int)\n",
    "\n",
    "X_train = np.stack(strat_train_text_[\"BERT_Embedding\"].values)\n",
    "X_val = np.stack(strat_train_text_val[\"BERT_Embedding\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_text_[\"BERT_Embedding\"].apply(lambda x: np.shape(x))\n",
    "y_train_cat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building of neural network for the model and compiling it and training it with the taining data set with the respective loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_loss import CategoricalFocalLoss\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "metrics = [tf.keras.metrics.Accuracy(),\n",
    "           tf.keras.metrics.Precision(),\n",
    "           tf.keras.metrics.Recall(),\n",
    "           tf.keras.metrics.F1Score()]\n",
    "\n",
    "nn_model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=[512,]),\n",
    "                                tf.keras.layers.Dense(100,activation=\"relu\"),\n",
    "                                tf.keras.layers.Dense(100,activation=\"relu\"),\n",
    "                                tf.keras.layers.Dense(43,activation=\"softmax\")])\n",
    "\n",
    "nn_model.compile(loss=CategoricalFocalLoss(alpha=1.0, gamma=2.0), optimizer=optimizer, metrics=metrics)\n",
    "nn_model.fit(X_train,y_train_cat,\n",
    "             epochs=30,batch_size=40,validation_data=(X_val,y_val_cat),\n",
    "             class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.keras.metrics.F1Score(nn_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
