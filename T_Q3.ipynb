{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_chunk_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    raw_text = \"\"\n",
    "\n",
    "    for page in reader.pages:\n",
    "        if page.extract_text():\n",
    "            raw_text += page.extract_text()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_text(raw_text)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def create_faiss_index(chunks):\n",
    "    docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    return db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query, db, k=15):\n",
    "    results = db.similarity_search(query, k=k)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "import requests\n",
    "\n",
    "GROQ_API_KEY = \"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\"  \n",
    "\n",
    "def generate_answer_groq(query, context, model=\"llama3-8b-8192\"):\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    system_prompt = \"You are a helpful assistant that answers questions based only on the provided context.\"\n",
    "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    try:\n",
    "        data = response.json()\n",
    "        if \"choices\" in data:\n",
    "            return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            return f\"⚠️ Unexpected response: {data}\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error: {e} | Raw response: {response.text}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chatbot(query, db):\n",
    "    docs = retrieve_docs(query, db)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    answer = generate_answer_groq(query, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second technical question is:\n",
      "\n",
      "1.2 Q2: Transfer Learning for Fashion-MNIST\n",
      "\n",
      "Objective: Adapt a pretrained CNN (e.g., ResNet50, VGG16) to classify 28 ×28 grayscale Fashion-MNIST images into 10 classes.\n",
      "\n",
      "Implementation:\n",
      "\n",
      "a) Data pipeline:\n",
      "\n",
      "* Resize to 224 ×224.\n",
      "* Convert 1 →3 channels (duplication or learnable adapter).\n",
      "\n",
      "b) Model:\n",
      "\n",
      "* Load pretrained backbone without top layers.\n",
      "* Freeze backbone; add new FC head.\n",
      "* Train head only; record validation metrics.\n",
      "\n",
      "c) Fine-tuning:\n",
      "\n",
      "* Unfreeze selected deeper blocks.\n",
      "\n",
      "Note: The goal is to adapt a pre-trained CNN to classify Fashion-MNIST images, using transfer learning. The implementation involves resizing the images, converting the channels, loading the pre-trained backbone, adding a new FC head, and training the head only. The fine-tuning step involves unfreezing selected deeper blocks.\n"
     ]
    }
   ],
   "source": [
    "chunks = load_and_chunk_pdf(\"/Users/pmanthan/Desktop/AICommunity_Assignment_25.pdf\")\n",
    "db = create_faiss_index(chunks)\n",
    "\n",
    "response = rag_chatbot(\"what is the second technical question in detail\",db)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
