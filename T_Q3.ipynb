{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) over PDF\n",
    "#### The code snippet creates a function which loads the pdf and reads and the chunks it into smalls chunks for for the next process embeddings to take place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_chunk_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    raw_text = \"\"\n",
    "\n",
    "    for page in reader.pages:\n",
    "        if page.extract_text():\n",
    "            raw_text += page.extract_text()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_text(raw_text)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After converting into embeddings it stores the embeddings into FAISS vectoebase data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def create_faiss_index(chunks):\n",
    "    docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    return db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code creates a function whihc takes out the process of retrieval of the data from database and re ranks it according to the importance of the data for the main LLM to carry out further process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query, db, k=15):\n",
    "    results = db.similarity_search(query, k=k)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code loads the main LLM whih is Groq in this case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "import requests\n",
    "\n",
    "GROQ_API_KEY = \"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\"  \n",
    "\n",
    "def generate_answer_groq(query, context, model=\"llama3-8b-8192\"):\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    system_prompt = \"You are a helpful assistant that answers questions based only on the provided context.\"\n",
    "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    try:\n",
    "        data = response.json()\n",
    "        if \"choices\" in data:\n",
    "            return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            return f\"⚠️ Unexpected response: {data}\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error: {e} | Raw response: {response.text}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below function calls all the pipeline function for the process to take place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chatbot(query, db):\n",
    "    docs = retrieve_docs(query, db)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    answer = generate_answer_groq(query, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following snippet of code call the load and chunk function and also the pipeline combinig function and also takes in the query from the user and genrates the answer for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second technical question is:\n",
      "\n",
      "1.2 Q2: Transfer Learning for Fashion-MNIST\n",
      "\n",
      "Objective: Adapt a pretrained CNN (e.g., ResNet50, VGG16) to classify 28 ×28 grayscale Fashion-MNIST images into 10 classes.\n",
      "\n",
      "Implementation:\n",
      "\n",
      "a) Data pipeline:\n",
      "\n",
      "* Resize to 224 ×224.\n",
      "* Convert 1 →3 channels (duplication or learnable adapter).\n",
      "\n",
      "b) Model:\n",
      "\n",
      "* Load pretrained backbone without top layers.\n",
      "* Freeze backbone; add new FC head.\n",
      "* Train head only; record validation metrics.\n",
      "\n",
      "c) Fine-tuning:\n",
      "\n",
      "* Unfreeze selected deeper blocks.\n",
      "\n",
      "Note that this question is asking you to adapt a pre-trained CNN to classify Fashion-MNIST images, which is a classic transfer learning problem. You need to resize the images, convert the channels, load the pre-trained backbone, add a new FC head, and fine-tune the model.\n"
     ]
    }
   ],
   "source": [
    "chunks = load_and_chunk_pdf(\"/Users/pmanthan/Desktop/AICommunity_Assignment_25.pdf\")\n",
    "db = create_faiss_index(chunks)\n",
    "\n",
    "response = rag_chatbot(\"what is the second technical question in detail\",db)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
