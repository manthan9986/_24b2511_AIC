{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) over PDF\n",
    "#### The code snippet creates a function which loads the pdf and reads and the chunks it into smalls chunks for for the next process embeddings to take place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_chunk_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    raw_text = \"\"\n",
    "\n",
    "    for page in reader.pages:\n",
    "        if page.extract_text():\n",
    "            raw_text += page.extract_text()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_text(raw_text)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After converting into embeddings it stores the embeddings into FAISS vectoebase data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def create_faiss_index(chunks):\n",
    "    docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    return db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code creates a function whihc takes out the process of retrieval of the data from database and re ranks it according to the importance of the data for the main LLM to carry out further process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query, db, k=15):\n",
    "    results = db.similarity_search(query, k=k)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code loads the main LLM whih is Groq in this case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "import requests\n",
    "\n",
    "GROQ_API_KEY = \"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\"  \n",
    "\n",
    "def generate_answer_groq(query, context, model=\"llama3-8b-8192\"):\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    system_prompt = \"You are a helpful assistant that answers questions based only on the provided context.\"\n",
    "    user_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    try:\n",
    "        data = response.json()\n",
    "        if \"choices\" in data:\n",
    "            return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            return f\"⚠️ Unexpected response: {data}\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error: {e} | Raw response: {response.text}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below function calls all the pipeline function for the process to take place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chatbot(query, db):\n",
    "    docs = retrieve_docs(query, db)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    answer = generate_answer_groq(query, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following snippet of code call the load and chunk function and also the pipeline combinig function and also takes in the query from the user and genrates the answer for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Unexpected response: {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "chunks = load_and_chunk_pdf(\"/Users/pmanthan/Desktop/AICommunity_Assignment_25.pdf\")\n",
    "db = create_faiss_index(chunks)\n",
    "\n",
    "response = rag_chatbot(\"what is the second technical question in detail\",db)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
